{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c30f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datasets import Dataset\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, pipeline\n",
    "import keras_hub\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "# Only log error messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f861fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The percentage of the dataset you want to split as train and test\n",
    "TRAIN_TEST_SPLIT = 0.1\n",
    "\n",
    "MAX_INPUT_LENGTH = 384  # Maximum length of the input to the model\n",
    "MAX_TARGET_LENGTH = 48  # Maximum length of the output by the model\n",
    "MIN_TARGET_LENGTH = 5   # Minimum length of the output by the model\n",
    "BATCH_SIZE = 8          # Batch-size for training our model\n",
    "LEARNING_RATE = 0.001   # Learning-rate for training our model\n",
    "MAX_EPOCHS = 1          # Maximum number of epochs we will train the model for\n",
    "\n",
    "# This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub\n",
    "MODEL_CHECKPOINT = \"t5-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92662867",
   "metadata": {},
   "source": [
    "These cells imports all necessary libraries and frameworks, sets up logging and environment configurations, and defines global constants for dataset splitting and model hyperparameters. It prepares the T5-small checkpoint and training parameters for a sequence-to-sequence learning task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043fd24",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a3484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 records\n"
     ]
    }
   ],
   "source": [
    "# Number of records to load from the JSON snapshot\n",
    "N = 200_000\n",
    "\n",
    "records = []\n",
    "\n",
    "# Read and parse up to N lines from the ArXiv metadata file\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Stop once we've read N records\n",
    "        if i >= N:\n",
    "            break\n",
    "        # Skip blank lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        # Parse each non-empty line as JSON and add to the list\n",
    "        records.append(json.loads(line))\n",
    "\n",
    "# Build a DataFrame, keep only the 'id', 'title', and 'abstract' columns,\n",
    "# drop any rows with missing values, and reset the index\n",
    "df = (\n",
    "    pd.DataFrame(records)\n",
    "      .loc[:, [\"id\", \"title\", \"abstract\"]]\n",
    "      .dropna()\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Confirm how many records were successfully loaded\n",
    "print(f\"Loaded {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ad6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame into a Hugging Face Dataset\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "# Rename columns for the seq2seq task:\n",
    "# - 'abstract' becomes the input to the model\n",
    "# - 'title' becomes the target output\n",
    "ds = ds.rename_column(\"abstract\", \"input_text\")\n",
    "ds = ds.rename_column(\"title\",    \"target_text\")\n",
    "\n",
    "# Split the dataset into training and test sets using the predefined ratio\n",
    "splits = ds.train_test_split(test_size=TRAIN_TEST_SPLIT, seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "test_ds  = splits[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368a2c2",
   "metadata": {},
   "source": [
    "This cell loads the first 200,000 ArXiv records from a JSON snapshot into a pandas DataFrame, filters and cleans the data, converts it into a Hugging Face Dataset with appropriately named input and target columns, and performs a reproducible train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a1c9f",
   "metadata": {},
   "source": [
    "# Load tokenizer, model, and collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a52a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained tokenizer and sequence-to-sequence model from the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Configure a data collator to handle dynamic padding for TensorFlow batches\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1c60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For T5 models, prepend a task-specific prefix to the inputs\n",
    "if MODEL_CHECKPOINT.startswith(\"t5-\"):\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a20ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Add the prefix to each abstract\n",
    "    inputs = [prefix + txt for txt in examples[\"input_text\"]]\n",
    "    # Tokenize inputs with truncation and padding\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Tokenize target texts (titles) similarly\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Assign tokenized label IDs for the model's training objective\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e60b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 180000/180000 [01:53<00:00, 1590.72 examples/s]\n",
      "Map: 100%|██████████| 20000/20000 [00:12<00:00, 1619.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function over the dataset splits, removing original columns\n",
    "tokenized_splits = splits.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"id\", \"input_text\", \"target_text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363f02f",
   "metadata": {},
   "source": [
    "These cells loads the pretrained tokenizer and T5 model, configures the data collator and optional “summarize:” prefix, defines a preprocessing function to tokenize inputs and target summaries to fixed lengths, and maps this function over the train/test splits to produce TensorFlow-ready datasets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f603e00",
   "metadata": {},
   "source": [
    "# Build tf datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6510686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timothychristian/Desktop/MSBA/spring/545 ML/tf-env/lib/python3.11/site-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert the training split into a TensorFlow dataset:\n",
    "# - Use 'input_ids' and 'attention_mask' as inputs\n",
    "# - Use 'labels' as targets\n",
    "# - Shuffle for training, batch according to BATCH_SIZE, and apply the data collator\n",
    "train_dataset = tokenized_splits[\"train\"].to_tf_dataset(\n",
    "    columns    = [\"input_ids\", \"attention_mask\"],\n",
    "    label_cols = [\"labels\"],\n",
    "    shuffle    = True,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    collate_fn = data_collator,\n",
    ")\n",
    "\n",
    "# Convert the test split into a TensorFlow dataset for evaluation:\n",
    "# - No shuffling to preserve order, same batching and collator\n",
    "test_dataset = tokenized_splits[\"test\"].to_tf_dataset(\n",
    "    columns    = [\"input_ids\", \"attention_mask\"],\n",
    "    label_cols = [\"labels\"],\n",
    "    shuffle    = False,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    collate_fn = data_collator,\n",
    ")\n",
    "\n",
    "# Prepare a small subset of the test split for generation/inference:\n",
    "# - Shuffle once for randomness, select the first 200 examples\n",
    "# - Convert to a tf.data.Dataset without further shuffling\n",
    "generation_dataset = (\n",
    "    tokenized_splits[\"test\"]\n",
    "        .shuffle(seed=42)\n",
    "        .select(range(200))\n",
    "        .to_tf_dataset(\n",
    "            columns    = [\"input_ids\", \"attention_mask\"],\n",
    "            label_cols = [\"labels\"],\n",
    "            shuffle    = False,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            collate_fn = data_collator,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9d0e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the Adam optimizer (default parameters)\n",
    "model.compile(optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c189d",
   "metadata": {},
   "source": [
    "This cell transforms the tokenized train and test splits into TensorFlow datasets with appropriate batching, shuffling, and collator settings for training, evaluation, and inference, then compiles the model using the Adam optimizer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c027443",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d66aaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RougeL metric for sequence-level evaluation\n",
    "rouge_l = keras_hub.metrics.RougeL()\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    preds, labels = eval_predictions\n",
    "\n",
    "    # Convert tensors to NumPy and ensure integer token IDs\n",
    "    if hasattr(preds, \"numpy\"):\n",
    "        preds = preds.numpy()\n",
    "    preds = np.clip(preds.astype(np.int64), 0, tokenizer.vocab_size - 1)\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds.tolist(), skip_special_tokens=True)\n",
    "\n",
    "    if hasattr(labels, \"numpy\"):\n",
    "        labels = labels.numpy()\n",
    "    labels = np.where(labels < 0, tokenizer.pad_token_id, labels)\n",
    "    decoded_labels = tokenizer.batch_decode(labels.tolist(), skip_special_tokens=True)\n",
    "\n",
    "    # Compute RougeL F1\n",
    "    res = rouge_l(decoded_labels, decoded_preds)\n",
    "    return {\"RougeL\": float(res[\"f1_score\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44c8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras callback to compute RougeL during training/inference\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn,\n",
    "    eval_dataset=generation_dataset,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "callbacks = [metric_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af828bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.1614"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timothychristian/Desktop/MSBA/spring/545 ML/tf-env/lib/python3.11/site-packages/transformers/generation/tf_utils.py:836: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752015637.429716 3310612 service.cc:152] XLA service 0x310cbb4e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752015637.429730 3310612 service.cc:160]   StreamExecutor device (0): Host, Default Version\n",
      "2025-07-08 16:00:37.485121: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1752015637.884299 3310612 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-07-08 16:01:27.773392: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 2113s 4s/step - loss: 1.1614 - val_loss: 0.9201 - RougeL: 0.3343\n",
      "Epoch 2/3\n",
      "388/500 [======================>.......] - ETA: 32:02 - loss: 1.0515"
     ]
    }
   ],
   "source": [
    "# Train the model for 3 epochs, tracking RougeL on the validation set\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=3,\n",
    "    steps_per_epoch=500,\n",
    "    validation_steps=50,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "# Build a Hugging Face pipeline for summarization using our fine-tuned model\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747213d",
   "metadata": {},
   "source": [
    "These cells define and attach a custom RougeL callback to track sequence-level F1 scores during training, runs the training loop for three epochs, then constructs a summarization pipeline and demonstrates inference by generating and printing a title (“Work Function Algorithm for the k-server problem”) for one test abstract.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
